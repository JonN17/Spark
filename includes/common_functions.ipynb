{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20782580-100f-4c35-a7a8-fdf6b0c39870",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Importing commonly used functions...\n",
    "* ezView()\n",
    "* add_ingestion_date()\n",
    "* re_arrange_partition_column()\n",
    "* overwrite_partition()\n",
    "* df_column_to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94411a23-cb93-4c42-ad76-5fb300c0b31b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "# Because display isn't github friendly\n",
    "def ezView(df, n=5, m=8):\n",
    "    \"\"\"\n",
    "    Display the first n rows and the first m columns of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame to display.\n",
    "    n (int): Number of rows to display. Default is 5.\n",
    "    m (int): Number of columns to display. Default is 8.\n",
    "    \"\"\"\n",
    "    # Ensure that n and m are within the DataFrame's bounds\n",
    "    num_rows = df.count()\n",
    "    num_columns = len(df.columns)\n",
    "    n = min(n, num_rows)\n",
    "    m = min(m, num_columns)\n",
    "\n",
    "    # Select the first m columns and display the first n rows\n",
    "    selected_columns = df.select(df.columns[:m])\n",
    "    selected_columns.limit(n).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32d28744-cd20-4f18-9f3f-a41a6b7fd966",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "def add_ingestion_date(input_df):\n",
    "  output_df = input_df.withColumn(\"ingestion_date\", current_timestamp())\n",
    "  return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e46b0bd-cf38-472a-8586-6ea0436f0ae4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Makes sure a certain column is the last column\n",
    "\n",
    "If you type:\n",
    "```\n",
    "re_arrange_partition_column(df, race_id)\n",
    "```\n",
    "Then this function will rearrange the dataframe `df` so race_id comes in as the last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9836bfb6-b00f-4ff6-ae51-3b736006bcd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def re_arrange_partition_column(input_df, partition_column):\n",
    "  column_list = []\n",
    "  for column_name in input_df.schema.names:\n",
    "    if column_name != partition_column:\n",
    "      column_list.append(column_name)\n",
    "  column_list.append(partition_column)\n",
    "  output_df = input_df.select(column_list)\n",
    "  return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c000b754-cdeb-40fe-afa3-a484fee861f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Incremental Update Partition\n",
    "\n",
    "Incremental load logic\n",
    "* works with cutover file (migration friendly)\n",
    "* works with delta files\n",
    "* will over that specific partition if that partition exists\n",
    "* does not produce duplicates of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99039dc3-9bf5-4553-94dc-1bab9a2e7524",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def overwrite_partition(input_df, db_name, table_name, partition_column):\n",
    "  output_df = re_arrange_partition_column(input_df, partition_column)\n",
    "  spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "  if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n",
    "    output_df.write.mode(\"overwrite\").insertInto(f\"{db_name}.{table_name}\")\n",
    "  else:\n",
    "    output_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"parquet\").saveAsTable(f\"{db_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5453df1-1f64-4952-a32f-4556cec90331",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Incremental Update Based on Distinct List of Values\n",
    "* Pulls distinct values from a specified column\n",
    "* Return all distinct values as a python list\n",
    "* Used for incrimental updating a database\n",
    "\n",
    "Not as readable as append method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1808d06-7dab-43b0-b173-4c324608779d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def df_column_to_list(input_df, column_name):\n",
    "  df_row_list = input_df.select(column_name) \\\n",
    "                        .distinct() \\\n",
    "                        .collect()\n",
    "  \n",
    "  column_value_list = [row[column_name] for row in df_row_list]\n",
    "  return column_value_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b17d634-deb2-43f9-ac45-2a0935ef0f67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Delta Table Merge\n",
    "So this is the first time you've got no table. You're going to write the data to the table with the new data, it'll just work as it is. So there is nothing we need to do.\n",
    "\n",
    "Source: S22:v146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82fbcd54-b7a9-4ba0-8527-17b43d175f71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_delta_data(input_df, db_name, table_name, folder_path, merge_condition, partition_column):\n",
    "    # Enable dynamic partition pruning for optimization\n",
    "    spark.conf.set(\"spark.databricks.optimizer.dynamicPartitionPruning\", \"true\")\n",
    "\n",
    "    from delta.tables import DeltaTable\n",
    "\n",
    "    # Check if the table exists in the specified database\n",
    "    if spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\"):\n",
    "        # Access the Delta table from the specified folder path\n",
    "        deltaTable = DeltaTable.forPath(spark, f\"{folder_path}/{table_name}\")\n",
    "\n",
    "        # Perform merge operation\n",
    "        deltaTable.alias(\"tgt\").merge(\n",
    "            input_df.alias(\"src\"),\n",
    "            merge_condition) \\\n",
    "            .whenMatchedUpdateAll() \\  # Update all matching records\n",
    "            .whenNotMatchedInsertAll() \\  # Insert all records that do not match\n",
    "            .execute()\n",
    "    else:\n",
    "        # If the table doesn't exist, write the DataFrame as a new Delta table\n",
    "        # with partitioning\n",
    "        input_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"delta\").saveAsTable(f\"{db_name}.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "common_functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e69faf-434d-477b-931a-8f479f7f5834",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Ingesting Multiple files\n",
    "\n",
    "* Ingesting multiple csv files\n",
    "* Ingesting multiple multi-line files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ce49b94-0b4a-49a4-9558-fb776d35f4c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Ingest lap_times folder\n",
    "\n",
    "This is for ingesting multiple files at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d26c36a-f4dd-45ee-8e1d-dfe521a04dad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Step 1 - Read the CSV file using the spark dataframe reader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "099a00db-5cd4-446c-86e9-0a1b74778566",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd749f65-b7d0-4e41-8306-c1d72b660db5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lap_times_schema = StructType(fields=[StructField(\"raceId\", IntegerType(), False),\n",
    "                                      StructField(\"driverId\", IntegerType(), True),\n",
    "                                      StructField(\"lap\", IntegerType(), True),\n",
    "                                      StructField(\"position\", IntegerType(), True),\n",
    "                                      StructField(\"time\", StringType(), True),\n",
    "                                      StructField(\"milliseconds\", IntegerType(), True)\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d149d077-7660-475f-9fae-d0f22645d80d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Import an entire folder of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bd0c6d1-703a-4b74-ad69-bb5e5e21d65f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lap_times_df = spark.read \\\n",
    ".schema(lap_times_schema) \\\n",
    ".csv(\"/mnt/formula1dl/raw/lap_times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "419d8597-9176-4110-971e-da6852b6f3b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Step 2 - Rename columns and add new columns\n",
    "1. Rename driverId and raceId\n",
    "1. Add ingestion_date with current timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f92f9547-9938-4e13-acb3-32abc028401e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac17a078-cd99-44cb-8766-4e8e7063673b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = lap_times_df.withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
    ".withColumnRenamed(\"raceId\", \"race_id\") \\\n",
    ".withColumn(\"ingestion_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "747c761f-9dea-46b7-8b1d-e7272b745926",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Step 3 - Write to output to processed container in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bad7d4a-32df-433a-8838-8bae46ef0ee0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write.mode(\"overwrite\").parquet(\"/mnt/formula1dl/processed/lap_times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f93eb527-c706-43f9-b130-9e07ff5c970d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Ingest qualifying json files\n",
    "\n",
    "Importing multiple multi-line JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30f5bc11-222c-4162-854d-a20d0c47f7c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Step 1 - Read the JSON file using the spark dataframe reader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cd8ea86-1da5-443a-84cf-48d92bf03a32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac9d2fa-96ab-4459-a120-609700c326dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qualifying_schema = StructType(fields=[StructField(\"qualifyId\", IntegerType(), False),\n",
    "                                      StructField(\"raceId\", IntegerType(), True),\n",
    "                                      StructField(\"driverId\", IntegerType(), True),\n",
    "                                      StructField(\"constructorId\", IntegerType(), True),\n",
    "                                      StructField(\"number\", IntegerType(), True),\n",
    "                                      StructField(\"position\", IntegerType(), True),\n",
    "                                      StructField(\"q1\", StringType(), True),\n",
    "                                      StructField(\"q2\", StringType(), True),\n",
    "                                      StructField(\"q3\", StringType(), True),\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19204fa-8bc8-4192-866e-83cc93bc8116",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qualifying_df = spark.read \\\n",
    ".schema(qualifying_schema) \\\n",
    ".option(\"multiLine\", True) \\\n",
    ".json(\"/mnt/formula1dl/raw/qualifying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca30a66e-2121-4999-be5d-df01cd94da7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Step 2 - Rename columns and add new columns\n",
    "1. Rename qualifyingId, driverId, constructorId and raceId\n",
    "1. Add ingestion_date with current timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f27ebe7-3cf0-4dc8-8f7d-2db4621970f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f86fe62-980a-4e18-9cbd-8fe5a51ce5e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = qualifying_df.withColumnRenamed(\"qualifyId\", \"qualify_id\") \\\n",
    ".withColumnRenamed(\"driverId\", \"driver_id\") \\\n",
    ".withColumnRenamed(\"raceId\", \"race_id\") \\\n",
    ".withColumnRenamed(\"constructorId\", \"constructor_id\") \\\n",
    ".withColumn(\"ingestion_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2506040-6d45-401d-9d56-c1d523782be3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Step 3 - Write to output to processed container in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8072f9a9-4aaf-4ac5-912e-198a945862f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write.mode(\"overwrite\").parquet(\"/mnt/formula1dl/processed/qualifying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec7015db-c6f3-404d-9b94-ac6eb13e505e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>qualify_id</th><th>race_id</th><th>driver_id</th><th>constructor_id</th><th>number</th><th>position</th><th>q1</th><th>q2</th><th>q3</th><th>ingestion_date</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "qualify_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "race_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "driver_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "constructor_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "number",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "position",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "q1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "q2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "q3",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ingestion_date",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.read.parquet('/mnt/formula1dl/processed/qualifying'))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkSkills LVL_1 Multiple File Ingestion via Databricks.ipynb",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
